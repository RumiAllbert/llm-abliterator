{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Induced Abliteration / Induction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abliterator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "## Check PyTorch version and GPU availability.\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Hugging Face Transformers library and other dependencies.\n",
    "!pip install -q transformers einops transformer_lens scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depedning on the model you plan to use, you may need to login to huggingface to download the model.\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model and tokenizer. Keep in mind the model has to be supported by transofmers lens.\n",
    "#\n",
    "model = ModelAbliterator(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    [\n",
    "        get_baseline_instructions(),\n",
    "        get_baseline_instructions(),\n",
    "    ],\n",
    "    activation_layers=[\"resid_pre\"],\n",
    ")\n",
    "\n",
    "# Blacklist the first and last layers\n",
    "model.blacklist_layer([0, 1, 2, 3, 29, 30, 31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Promp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ChatTemplate\n",
    "# Modify this as you wish. This is just an example.\n",
    "system_prompt = \"\"\"You are highly optimistic. Your responses should reflect a positive and hopeful outlook on life. Emphasize the bright side of any situation, and express strong confidence that things will turn out well. Encourage others with uplifting and encouraging language.\"\"\"\n",
    "chat_template = ChatTemplate(\n",
    "    model,\n",
    "    \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "    + system_prompt\n",
    "    + \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{instruction}<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Baseline behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the model responds as a baseline.\n",
    "model.test(\n",
    "    N=32,\n",
    "    test_set=model.baseline_inst_test[15:16],\n",
    "    max_tokens_generated=100,\n",
    "    drop_refusals=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and measure the effectiveness of our prompt\n",
    "with chat_template:\n",
    "    model.test(N=4, test_set=model.baseline_inst_test[30:33], drop_refusals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the activation direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define file paths\n",
    "MODEL = \"llama3\"\n",
    "# MODEL = \"phi3\"\n",
    "\n",
    "## You can use the following to save baseline and altered caches.\n",
    "\n",
    "baseline_cache_path = f\"output/baseline_cache_{MODEL}_compressed.pkl.gz\"\n",
    "\n",
    "# ==================================\n",
    "# Main Process\n",
    "# ==================================\n",
    "# Calculate baseline_cache once if it doesn't exist\n",
    "if not os.path.exists(baseline_cache_path):\n",
    "    print(\"Calculating baseline cache...\")\n",
    "\n",
    "    # Define prompt count\n",
    "    prompt_count = 1500  # using more samples can better target the direction\n",
    "\n",
    "    # Tokenize instructions for baseline\n",
    "    baseline = model.tokenize_instructions_fn(\n",
    "        model.baseline_inst_train[:prompt_count]\n",
    "    )  # Use base system prompt\n",
    "\n",
    "    # Get baseline cache\n",
    "    baseline_cache = model.create_activation_cache(baseline, N=len(baseline))\n",
    "    base_cache, _ = baseline_cache\n",
    "\n",
    "    # Save baseline cache\n",
    "    save_compressed_cache(base_cache, baseline_cache_path)\n",
    "\n",
    "else:\n",
    "    print(\"Baseline cache already exists.\")\n",
    "\n",
    "# Load baseline cache\n",
    "baseline_cache = load_compressed_cache(baseline_cache_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with chat_template:\n",
    "    # Tokenize instructions for altered tokens\n",
    "    altered_toks = model.tokenize_instructions_fn(\n",
    "        model.baseline_inst_train[:prompt_count]\n",
    "    )\n",
    "\n",
    "altered_cache = model.create_activation_cache(altered_toks, N=len(altered_toks))\n",
    "\n",
    "# Set trait and baseline caches\n",
    "model.trait, _ = altered_cache\n",
    "model.baseline = baseline_cache\n",
    "\n",
    "# Get feature directions\n",
    "feature_directions = model.refusal_dirs(\n",
    "    invert=True\n",
    ")  # inverted because we're attempting to induce the feature, otherwise it would be a refusal direction\n",
    "\n",
    "# And now let's find the direction that best expresses the desired behaviour!\n",
    "modifier = 1.3\n",
    "# If the model is not behaving as expected, try changing the modifier value. Lower is more stable.\n",
    "\n",
    "for block in feature_directions:\n",
    "    with model:  # this line makes it so any changes we apply to the model's weights will be reverted on each loop\n",
    "        model.apply_refusal_dirs([feature_directions[block] * modifier])\n",
    "        print(block)\n",
    "\n",
    "        model.test(\n",
    "            N=32,\n",
    "            test_set=model.baseline_inst_test[15:25],\n",
    "            max_tokens_generated=64,\n",
    "            drop_refusals=False,\n",
    "        )\n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see which layer gives the most desireble output. I've found 17 or 18 are the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply_refusal_dirs([feature_directions[\"blocks.18.hook_resid_pre\"] * modifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More testing on modified model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(\n",
    "    N=32,\n",
    "    test_set=model.baseline_inst_test[15:25],\n",
    "    max_tokens_generated=64,\n",
    "    drop_refusals=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = model.model.cfg\n",
    "state_dict = model.model.state_dict()\n",
    "\n",
    "# load the original model as a regular unhooked Transformer -- don't need to load it into GPU as it's just for saving\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model.MODEL_PATH, torch_dtype=torch.bfloat16\n",
    ")\n",
    "lm_model = hf_model.model  # get the language model component\n",
    "\n",
    "for l in range(cfg.n_layers):\n",
    "    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(\n",
    "        einops.rearrange(\n",
    "            state_dict[f\"blocks.{l}.attn.W_O\"], \"n h m->m (n h)\", n=cfg.n_heads\n",
    "        ).contiguous()\n",
    "    )\n",
    "    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(\n",
    "        torch.transpose(state_dict[f\"blocks.{l}.mlp.W_out\"], 0, 1).contiguous()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to the hub\n",
    "hf_model.push_to_hub(\"your-model-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T19:21:33.696839Z",
     "iopub.status.busy": "2024-06-28T19:21:33.696422Z",
     "iopub.status.idle": "2024-06-28T19:22:09.128878Z",
     "shell.execute_reply": "2024-06-28T19:22:09.127954Z",
     "shell.execute_reply.started": "2024-06-28T19:21:33.696809Z"
    }
   },
   "outputs": [],
   "source": [
    "# To save locally\n",
    "hf_model.save_pretrained(\"your model name\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5287510,
     "sourceId": 8793862,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5287588,
     "sourceId": 8793973,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "blind-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
